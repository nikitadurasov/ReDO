{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRq8Ils_gmyt"
   },
   "source": [
    "# Does ReDO learn classification ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rSdv4Vfnu-W4"
   },
   "source": [
    "To run this notebook you have to have following libraries installed (tested with specified verisinons):\n",
    "\n",
    " * numpy == 1.17.4\n",
    "\n",
    " * scipy == 1.3.3\n",
    "\n",
    " * PIL == 4.3.0\n",
    "\n",
    " * sklearn == 0.21.3\n",
    "\n",
    " * pytorch == 1.3.1\n",
    "\n",
    "The directories should be organized as follows: \n",
    " \n",
    "1) In the same directory with this notebook there should be downloaded repositoriy of [ReDO](https://github.com/mickaelChen/ReDO.git). \n",
    " \n",
    "2) Download and extract: Dataset, Segmentations, Image labels and data splits from http://www.robots.ox.ac.uk/~vgg/data/flowers/102/. The obtained jpg folder, segmin folder and setid.mat file should be placed in the folder 'data/flowers'.\n",
    "\n",
    "3) Download and unzip weigth [dataset_nets_state.tar.gz](https://drive.google.com/drive/folders/1hUb2iOTJAbWw1NotWGAsEt4ASomhOwbh) into 'weights' folder\n",
    "\n",
    " We provide script \"prepare_working_directory.sh\" which does everything **except step 3**, which need to be done manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uy1JNjwLgrWu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "from PIL import Image\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torchvision\n",
    "from  torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "import ReDO.models as models\n",
    "import ReDO.datasets as datasets\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "DATAPATH = 'data/flowers'\n",
    "WEIGHTPATH = 'weights'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tLsgY3fS3tGy"
   },
   "source": [
    "Here we redefine classes of some Neural Networks provided by ReDO and the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JnD-B3USm6nX"
   },
   "outputs": [],
   "source": [
    "# We redefine class of segmentator \n",
    "# to make it return embeddings from the middle part of the network\n",
    "class _netEncM(nn.Module):\n",
    "    def __init__(self, sizex=128, nIn=3, nMasks=2, nRes=5, nf=128, temperature=1):\n",
    "        super(_netEncM, self).__init__()\n",
    "        self.nMasks = nMasks\n",
    "        sizex = sizex // 4 \n",
    "        self.cnn = nn.Sequential(*([models._downConv(nIn, nf)] +\n",
    "                                   [models._resBloc(nf=nf) for i in range(nRes)]))\n",
    "        self.psp = nn.ModuleList([nn.Sequential(nn.AvgPool2d(sizex),\n",
    "                                                nn.Conv2d(nf,1,1),\n",
    "                                                nn.Upsample(size=sizex, mode='bilinear')),\n",
    "                                  nn.Sequential(nn.AvgPool2d(sizex//2, sizex//2),\n",
    "                                                nn.Conv2d(nf,1,1),\n",
    "                                                nn.Upsample(size=sizex, mode='bilinear')),\n",
    "                                  nn.Sequential(nn.AvgPool2d(sizex//3, sizex//3),\n",
    "                                                nn.Conv2d(nf,1,1),\n",
    "                                                nn.Upsample(size=sizex, mode='bilinear')),\n",
    "                                  nn.Sequential(nn.AvgPool2d(sizex//6, sizex//6),\n",
    "                                                nn.Conv2d(nf,1,1),\n",
    "                                                nn.Upsample(size=sizex, mode='bilinear'))])\n",
    "        self.out = models._upConv(1 if nMasks == 2 else nMasks, nf+4)\n",
    "        self.temperature = temperature\n",
    "    def forward(self, x):\n",
    "        f = self.cnn(x)\n",
    "        # m = self.out(torch.cat([f] + [pnet(f) for pnet in self.psp], 1))\n",
    "        # if self.nMasks == 2:\n",
    "        #     m = torch.sigmoid(m / self.temperature)\n",
    "        #     m = torch.cat((m, (1-m)), 1)\n",
    "        # else:\n",
    "        #     m = F.softmax(m / self.temperature, dim=1)\n",
    "        return f\n",
    "\n",
    "# We redefine class of the Discriminator to make it\n",
    "# return embeddings from the last FC layerinstead of real/fake image prediction\n",
    "class _resDiscriminator128(nn.Module):\n",
    "    def __init__(self, nIn=3, nf=64, selfAtt=False):\n",
    "        super(_resDiscriminator128, self).__init__()\n",
    "        self.blocs = []\n",
    "        self.sc = []\n",
    "        # first bloc\n",
    "        self.bloc0 = nn.Sequential(spectral_norm(nn.Conv2d(nIn, nf, 3, 1, 1, bias=True)),\n",
    "                                   nn.ReLU(),\n",
    "                                   spectral_norm(nn.Conv2d(nf, nf, 3, 1, 1, bias=True)),\n",
    "                                   nn.AvgPool2d(2),)\n",
    "        self.sc0 = nn.Sequential(nn.AvgPool2d(2),\n",
    "                                 spectral_norm(nn.Conv2d(nIn, nf, 1, bias=True)),)\n",
    "        if selfAtt:\n",
    "            self.selfAtt = models.SelfAttention(nf)\n",
    "        else:\n",
    "            self.selfAtt = nn.Sequential()\n",
    "        # Down blocs\n",
    "        for i in range(4):\n",
    "            nfPrev = nf\n",
    "            nf = nf*2\n",
    "            self.blocs.append(nn.Sequential(nn.ReLU(),\n",
    "                                            spectral_norm(nn.Conv2d(nfPrev, nf, 3, 1, 1, bias=True)),\n",
    "                                            nn.ReLU(),\n",
    "                                            spectral_norm(nn.Conv2d(nf, nf, 3, 1, 1, bias=True)),\n",
    "                                            nn.AvgPool2d(2),))\n",
    "            self.sc.append(nn.Sequential(nn.AvgPool2d(2),\n",
    "                                         spectral_norm(nn.Conv2d(nfPrev, nf, 1, bias=True)),))\n",
    "        # Last Bloc\n",
    "        self.blocs.append(nn.Sequential(nn.ReLU(),\n",
    "                                        spectral_norm(nn.Conv2d(nf, nf, 3, 1, 1, bias=True)),\n",
    "                                        nn.ReLU(),\n",
    "                                        spectral_norm(nn.Conv2d(nf, nf, 3, 1, 1, bias=True))))\n",
    "        self.sc.append(nn.Sequential())\n",
    "        self.dense = nn.Linear(nf, 1)\n",
    "        self.blocs = nn.ModuleList(self.blocs)\n",
    "        self.sc = nn.ModuleList(self.sc)\n",
    "    def forward(self, x):\n",
    "        x = self.selfAtt(self.bloc0(x) + self.sc0(x))\n",
    "        for k in range(len(self.blocs)):\n",
    "            x = self.blocs[k](x) + self.sc[k](x)\n",
    "        x = x.sum(3).sum(2)\n",
    "        return x\n",
    "\n",
    "\n",
    "# We redefine class of Dataset to be able to store labels of images\n",
    "class FlowersDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataPath, sets='train', transform=transforms.ToTensor()):\n",
    "        super(FlowersDataset, self).__init__()\n",
    "        self.files =  io.loadmat(os.path.join(dataPath, \"setid.mat\"))\n",
    "        self.labels = io.loadmat(os.path.join(dataPath, \"imagelabels.mat\")).get('labels')[0]\n",
    "        # if sets == 'train':\n",
    "        #    self.files = self.files.get('tstid')[0]\n",
    "        # elif sets == 'val':\n",
    "        #    self.files = self.files.get('valid')[0]\n",
    "        # else:\n",
    "        #    self.files = self.files.get('trnid')[0]\n",
    "        self.transform = transform\n",
    "        self.datapath = dataPath\n",
    "    def __len__(self):\n",
    "        return len(self.files.get('tstid')[0]) + \\\n",
    "               len(self.files.get('valid')[0]) + \\\n",
    "               len(self.files.get('trnid')[0])\n",
    "    def __getitem__(self, idx):\n",
    "        imgname = \"image_%05d.jpg\" % (idx + 1)\n",
    "        segname = \"segmim_%05d.jpg\" % (idx + 1)\n",
    "        label   = self.labels[idx]\n",
    "        img = self.transform(Image.open(os.path.join(self.datapath, \"jpg\", imgname)))\n",
    "        seg = np.array(Image.open(os.path.join(self.datapath, \"segmim\", segname)))\n",
    "        seg = 1 - ((seg[:,:,0:1] == 0) + (seg[:,:,1:2] == 0) + (seg[:,:,2:3] == 254))\n",
    "        seg = (seg * 255).astype('uint8').repeat(3,axis=2)\n",
    "        seg = self.transform(Image.fromarray(seg))[:1]\n",
    "        return img * 2 - 1, seg, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "79tcE4Ac5UTY"
   },
   "source": [
    "Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uOWfvEJviRCn"
   },
   "outputs": [],
   "source": [
    "states = torch.load(WEIGHTPATH, map_location={'cuda:0' : 'cuda:0'})\n",
    "opt = states['options']\n",
    "if \"netEncM\" in states:\n",
    "    netEncM = _netEncM(sizex=opt.sizex, nIn=opt.nx, nMasks=opt.nMasks, nRes=opt.nResM, nf=opt.nfM, temperature=opt.temperature).to(device)\n",
    "    netEncM.load_state_dict(states[\"netEncM\"])\n",
    "    netEncM.eval()\n",
    "if \"netGenX\" in states:\n",
    "    netGenX = models._netGenX(sizex=opt.sizex, nOut=opt.nx, nc=opt.nz, nf=opt.nfX, nMasks=opt.nMasks, selfAtt=opt.useSelfAttG).to(device)\n",
    "    netGenX.load_state_dict(states[\"netGenX\"])\n",
    "    netGenX.eval()\n",
    "if \"netRecZ\" in states:\n",
    "    netRecZ = models._netRecZ(sizex=opt.sizex, nIn=opt.nx, nc=opt.nz, nf=opt.nfZ, nMasks=opt.nMasks).to(device)\n",
    "    netRecZ.load_state_dict(states[\"netRecZ\"])\n",
    "    netRecZ.eval()\n",
    "if \"netDX\" in states:\n",
    "    netDX = _resDiscriminator128(nIn=opt.nx, nf=opt.nfD, selfAtt=opt.useSelfAttD).to(device)\n",
    "    netDX.load_state_dict(states[\"netDX\"])\n",
    "    netDX.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UbaHjJa55mE9"
   },
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YabVKF12iqs1"
   },
   "outputs": [],
   "source": [
    "dataset = FlowersDataset(DATAPATH, \"train\",\n",
    "            torchvision.transforms.Compose([torchvision.transforms.Resize(opt.sizex, Image.NEAREST),\n",
    "                                            torchvision.transforms.CenterCrop(opt.sizex),\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "            ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NbtghByk5n8t"
   },
   "source": [
    "Create a function to automatize evaluation. The function takes callback function which takes data sample and computes embeddings. Then it trains KNN on the specified number of batches and compute accuracy on the rest of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LtsTeSUzw-k5"
   },
   "outputs": [],
   "source": [
    "def evaluate_embedding(get_embedding, batch_size=100, batches_for_train=2):\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    # Compute embeddings\n",
    "    train_X, train_y = get_embedding(loader, n_batches=batches_for_train)\n",
    "    test_X, test_y = get_embedding(loader, n_batches=len(loader) - batches_for_train)\n",
    "    print('Sizes are: ', train_X.size(), test_X.size())\n",
    "\n",
    "    # Train KNN\n",
    "    clf = neighbors.KNeighborsClassifier(1, weights='distance')\n",
    "    clf.fit(train_X, train_y)\n",
    "\n",
    "    # Compute accuracy\n",
    "    pred_y = clf.predict(test_X)\n",
    "    accuracy = accuracy_score(test_y, pred_y)\n",
    "    print('Accuracy: ', accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LjKt9cMq6uiW"
   },
   "source": [
    "# Expirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GpJfBBOyLig"
   },
   "source": [
    "## Raw Images KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SIq3Q1J0J88"
   },
   "outputs": [],
   "source": [
    "def get_data_raw(loader, n_batches=1):\n",
    "    labels = []\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "      for i in range(n_batches):\n",
    "        xData, mData, batch_labels = next(iter(loader))\n",
    "        xData = xData.to(device)\n",
    "        mData = mData.to(device)\n",
    "        batch_embeddings = netEncM(xData).reshape(100, -1)\n",
    "\n",
    "        labels.append(batch_labels)\n",
    "        embeddings.append(batch_embeddings)\n",
    "\n",
    "    return torch.cat(embeddings, dim=0).cpu() , torch.cat(labels, dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CzhIytZR0XyM"
   },
   "outputs": [],
   "source": [
    "evaluate_embedding(get_data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ibm6_cFSGoxZ"
   },
   "source": [
    "## Raw images resized to 3x16x16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pme0X3O_GoEm"
   },
   "outputs": [],
   "source": [
    "def get_data_raw_resized(loader, n_batches=1):\n",
    "    labels = []\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_batches):\n",
    "            xData, mData, batch_labels = next(iter(loader))\n",
    "            xData = xData.to(device)\n",
    "            mData = mData.to(device)\n",
    "            batch_embeddings = F.max_pool2d(xData, 8).reshape(100, -1)\n",
    "\n",
    "            labels.append(batch_labels)\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "    return torch.cat(embeddings, dim=0).cpu() , torch.cat(labels, dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDvdRi6cHuRA"
   },
   "outputs": [],
   "source": [
    "evaluate_embedding(get_data_raw_resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C1tKjDSRy74r"
   },
   "source": [
    "## KNN on Segmentation embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DNfdUI7eySgv"
   },
   "outputs": [],
   "source": [
    "def get_data_from_segm(loader, n_batches=1):\n",
    "    labels = []\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_batches):\n",
    "            xData, mData, batch_labels = next(iter(loader))\n",
    "            xData = xData.to(device)\n",
    "            mData = mData.to(device)\n",
    "            batch_embeddings = netEncM(xData).reshape(100, -1)\n",
    "\n",
    "            labels.append(batch_labels)\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "\n",
    "    return torch.cat(embeddings, dim=0).cpu() , torch.cat(labels, dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nm7EVU0K0FqP"
   },
   "outputs": [],
   "source": [
    "evaluate_embedding(get_data_from_segm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2iF79oStwuix"
   },
   "source": [
    "## KNN on Discriminator Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U6gYS-cmqynG"
   },
   "outputs": [],
   "source": [
    "def get_data_from_descr(loader, n_batches=1):\n",
    "    labels = []\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_batches):\n",
    "            xData, mData, batch_labels = next(iter(loader))\n",
    "            xData = xData.to(device)\n",
    "            mData = mData.to(device)\n",
    "            batch_embeddings = netDX(xData)\n",
    "\n",
    "            labels.append(batch_labels)\n",
    "            embeddings.append(batch_embeddings) #xData.reshape(100, -1))\n",
    "\n",
    "\n",
    "    return torch.cat(embeddings, dim=0).cpu() , torch.cat(labels, dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WnuRyL59xKBV",
    "outputId": "ded71fa9-b7ed-4068-c168-75f6fb7ca4fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.229625"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_embedding(get_data_from_descr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below code is redundant and we didn't use it in our report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sgkboMQE4NET"
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, code_size):\n",
    "        super().__init__()\n",
    "        self.code_size = code_size\n",
    "        # Encoder\n",
    "        self.enc_cnn_1 = nn.Conv2d(3, 10, kernel_size=5)\n",
    "        self.enc_cnn_2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.enc_cnn_3 = nn.Conv2d(20, 30, kernel_size=5)\n",
    "        self.enc_linear_1 = nn.Linear(30 * 12 * 12, 2048)\n",
    "        self.enc_linear_2 = nn.Linear(2048, self.code_size)\n",
    "        # Decoder\n",
    "        self.dec_linear_1 = nn.Linear(self.code_size, 2048)\n",
    "        self.dec_linear_2 = nn.Linear(2048, 3* IMAGE_WIDTH * IMAGE_HEIGHT)\n",
    "    def forward(self, images):\n",
    "        code = self.encode(images)\n",
    "        out = self.decode(code)\n",
    "        return out, code\n",
    "    def encode(self, images):\n",
    "        code = self.enc_cnn_1(images)\n",
    "        code = F.selu(F.max_pool2d(code, 2))\n",
    "        code = self.enc_cnn_2(code)\n",
    "        code = F.selu(F.max_pool2d(code, 2))\n",
    "        code = self.enc_cnn_3(code)\n",
    "        code = F.selu(F.max_pool2d(code, 2))\n",
    "        code = code.view([images.size(0), -1])\n",
    "        code = F.selu(self.enc_linear_1(code))\n",
    "        code = self.enc_linear_2(code)\n",
    "        return code\n",
    "    def decode(self, code):\n",
    "        out = F.selu(self.dec_linear_1(code))\n",
    "        out = torch.sigmoid(self.dec_linear_2(out))\n",
    "        out = out.view([code.size(0), 3, IMAGE_WIDTH, IMAGE_HEIGHT])\n",
    "        return out\n",
    "\n",
    "IMAGE_WIDTH = IMAGE_HEIGHT = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ZMntknGBj63"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "code_size = 1024\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "optimizer_cls = optim.Adam\n",
    "# Load data\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "# Instantiate model\n",
    "autoencoder = AutoEncoder(code_size).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optimizer_cls(autoencoder.parameters(), lr=lr)\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch %d\" % epoch)\n",
    "    for i, (images, _, _) in enumerate(loader):    # Ignore image labels\n",
    "        out, code = autoencoder(Variable(images).to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(out, images.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Loss = %.3f\" % loss.data)\n",
    "  \n",
    "    # update LR\n",
    "    lr /= 10\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zrMOzRqZ_DGE"
   },
   "outputs": [],
   "source": [
    "def get_data_from_autoenc(loader, n_batches=1):\n",
    "    labels = []\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_batches):\n",
    "            xData, mData, batch_labels = next(iter(loader))\n",
    "            xData = xData.to(device)\n",
    "            mData = mData.to(device)\n",
    "            batch_embeddings = autoencoder.encode(xData)\n",
    "\n",
    "            labels.append(batch_labels)\n",
    "            embeddings.append(batch_embeddings) \n",
    "  \n",
    "  return torch.cat(embeddings, dim=0).cpu() , torch.cat(labels, dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "-1Wsz5o4_Z5t",
    "outputId": "c47afd54-6852-4cfe-d49a-6d37a5b1185a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes are:  torch.Size([256, 1024]) torch.Size([7936, 1024])\n",
      "Accuracy:  0.06149193548387097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06149193548387097"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_embedding(get_data_from_autoenc, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
