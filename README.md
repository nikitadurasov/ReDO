# ReDO: Unsupervised Object Segmentation by Redrawing (Reproducibility challenge)

## General
We investigated results from paper ReDO: Unsupervised Object Segmentation by Redrawing ([https://arxiv.org/abs/1905.13539](https://arxiv.org/abs/1905.13539)) from NeurIPS 2019. This folder contains code and scripts to reproduce the results which we presented in our report. If you will face any problems with this code or something will be unclear in our report, feel free to contact us via e-mails: [fedor.moiseev@epfl.ch](fedor.moiseev@epfl.ch), [nikita.durasov@epfl.ch](nikita.durasov@epfl.ch), [artem.lukoianov@epfl.ch](artem.lukoianov@epfl.ch).

## Code description
Our code is based on the authors' code which was provided with paper ([https://github.com/mickaelChen/ReDO](https://github.com/mickaelChen/ReDO)). Our contribution in code:
* **regularization.py** - this file is completely written by us and contains different mask regularizations that we described in our report.
* **helpers.py** - this file is completely written by us and contains some utility functions.
* **train.py** - this file is based on authors' train.py file (see author's repo) but we modified it to implement techniques which we used in our report (different mask regularizations, full image regeneration during training). It contains a training code for unsupervised segmentation. In this file parts of code that were added by us start with a comment line "*# BEGIN OF OUR CODE*" and end with a comment line "*# END OF OUR CODE*".
* **UnsupervisedSegmentationExperiments.ipynb** - this notebook contains experiments for unsupervised classification (Section V in our report) 
* **scripts/preprocessing/get_flowers.sh** - this file is completely written by us and contains a script which downloads Flowers dataset.
* **scripts/training/flowers** - all files in this folder are completely written by us (but based on the author's training script) and contain training scripts on Flowers dataset with different config. See the next section for details.

All other files are taken from the authors' repository without any modifications (please don't punish us for quality of authors' code ;))


## Guideline to reproduce our results
This section contains instructions to reproduce results that were reported by us. It requires GPU with at least 12GB RAM (i.e. GTX 1080TI). Each training script works for approximately 12 hours (depends on hardware and concrete script). All scripts (preprocessing and training) should be run from the main directory (for example, `./scripts/training/flowers/train_base_model.sh`).

### Prerequisites
We use Python 3.7.1 and the following libraries:
* numpy==1.15.4 - used by authors, we don't use it in our modifications.
* scipy==1.1.0 - used by authors, we don't use it in our modifications.
* torch==1.0.0 - used for Deep Neural Nets training ;)
* tqdm==4.40.2 - used by authors, we don't use it in our modifications.
* sklearn==0.21.3 - for unsupervised classification experiments.
If you need, you can install then with the command: `pip install -r requirements.txt`. Also please make sure that the main version of Python in your operating system is Python 3 (we suggest to use virtual environments).


Firstly, you need to run `./scripts/preprocessing/get_flowers.sh` to download Flowers dataset. Then `./scripts/training/flowers/train_base_model.sh` to train model from original paper on Flowers dataset. And finally you can run the following scripts to reproduce our results:
### Ablation study (Section III in our report)
* `./scripts/training/flowers/train_wrecZ_zeroed_model.sh` - trains model without Conservation of Region information loss term.
* `./scripts/training/flowers/train_wrecZ_zeroed_model.sh` - trains model without Conservation of Region information (CoR below) loss term with random seed (to ensure that it always fails).
* `./scripts/training/flowers/train_without_selfattD_model.sh` - trains model without self-attention in discriminator.
* `./scripts/training/flowers/train_without_selfattG_model.sh` - trains model without self-attention in generator.
* `./scripts/training/flowers/train_without_selfattGD_model.sh` - trains model without self-attention in both generator and discriminator.
* `./scripts/training/flowers/train_base_model_full_generator.sh` - trains model in setup where whole image is generated by generator instead of combining generated and original image.

### Mask regularization (Section IV in our report)
* `./scripts/training/flowers/train_mask_regularized_mean_mse.sh` - trains model without CoR loss term but with MSE regularization on masks.
* `./scripts/training/flowers/train_mask_regularized_rectified_mean_mse.sh` - trains model without CoR loss term but with Rectified MSE regularization on masks.
* `./scripts/training/flowers/train_mask_regularized_concrete_pdf.sh` - trains model without CoR loss term but with Concrete PDF regularization on masks.
* `./scripts/training/flowers/train_mask regularized_concrete_pdf_and_base.sh` - trains model with additional Concrete PDF regularization on masks during early training.
* `./scripts/training/flowers/train_mask_regularized_concrete_pdf_and_base_random_seed.sh` - trains model with additional Concrete PDF regularization on masks during early training with random seed (to ensure that it never fails and is more robust that original one).

Provided scripts by default contain hyperparameters that were used by us to provided results mentioned in our report, but we run it with different hyperparameters to find the good ones. If you also want to do it, you can manually change hyperparameters in this scripts (the most important one is `--reg_mask` (mask regularization coefficient), most of the others were just taken from the original paper)

### Unsupervised classification (Section V in our report)
For this notebook we used slightly different version of libraries (because we run it on google colab):
* numpy==1.17.4
* scipy==1.3.3
* PIL==4.3.0
* sklearn==0.21.3
* pytorch==1.3.1

We suppose that it will work fine with the versions provided above, but we didn't test it.

Firstly, you need to download and unzip weights [dataset_nets_state.tar.gz](https://drive.google.com/drive/folders/1hUb2iOTJAbWw1NotWGAsEt4ASomhOwbh) into 'weights' folder. Then you can just open **UnsupervisedClassificationExperiments.ipynb** notebook and run all cells to reproduce our results.


